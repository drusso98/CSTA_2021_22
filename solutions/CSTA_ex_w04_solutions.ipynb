{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex 1\n",
    "The function below is able to scrape all the reviews contained on a Trustpilot page (whose link is passed as parameter) and the store these information in a dictionary. Modify the function in order to scrape the <em>name</em> of the reviewer, the <em>location</em>, the <em>number of reviews</em> and the <em>title</em> of them. Moreover, scrape the replies if they exist, otherwise set the value at None. Save all the information in a <em>.csv</em> file (the path of the file must be the second argument of the function). Divide all the information with a semicolon \";\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trust_juicer(u):\n",
    "    source_ = requests.get(u).text\n",
    "    res_ = BeautifulSoup(source_).find_all(\"article\")\n",
    "    d = {}\n",
    "    for i,t in enumerate(res_):\n",
    "        try:\n",
    "            tempd = {}\n",
    "            text = t.find(\"p\").get_text()\n",
    "            label = t.find(\"div\", class_= \"star-rating star-rating--medium\").find(\"img\").attrs[\"alt\"][0]\n",
    "\n",
    "            tempd[\"text\"] = text.strip()\n",
    "            tempd[\"label\"] = label\n",
    "\n",
    "            d[i] = tempd\n",
    "        except Exception as e:\n",
    "            print(i, e)\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup \n",
    "\n",
    "tp_url = \"https://www.trustpilot.com/review/www.nzymes.com\"\n",
    "src = requests.get(tp_url).text\n",
    "soup = BeautifulSoup(src)\n",
    "\n",
    "def trust_juicer(u, path):\n",
    "    source_ = requests.get(u).text\n",
    "    soup = BeautifulSoup(source_)\n",
    "    res_ = soup.find_all(\"article\")\n",
    "    file = open(path, \"w\", encoding=\"utf-8\")\n",
    "    d = {}\n",
    "    for i,t in enumerate(res_):\n",
    "        try:\n",
    "            tempd = {}\n",
    "            text = t.find(\"p\").get_text()\n",
    "            label = t.find(\"div\", class_= \"star-rating star-rating--medium\").find(\"img\").attrs[\"alt\"][0]\n",
    "            name = t.find(\"div\", class_=\"consumer-information__name\").get_text()\n",
    "            location = t.find(\"div\", class_=\"consumer-information__location\").find(\"span\").get_text()\n",
    "            n_of_reviews = t.find(\"div\", class_=\"consumer-information__review-count\").find(\"span\").get_text()\n",
    "            title = t.find(\"h2\", class_=\"review-content__title\").get_text()\n",
    "            \n",
    "            try:\n",
    "                reply = t.find(\"div\", class_=\"brand-company-reply__content\").get_text()\n",
    "            except Exception as e:\n",
    "                reply = \"None\"\n",
    "            \n",
    "            tempd[\"location\"] = location\n",
    "            tempd[\"n_of_reviews\"] = n_of_reviews\n",
    "            tempd[\"name\"] = name.strip()\n",
    "            tempd[\"title\"] = title.strip()\n",
    "            tempd[\"text\"] = text.strip()\n",
    "            tempd[\"label\"] = label\n",
    "            tempd[\"reply\"] = reply.strip()\n",
    "            \n",
    "            d[i] = tempd\n",
    "            \n",
    "            line = tempd[\"name\"] + \";\" + tempd[\"location\"] + \";\" + tempd[\"n_of_reviews\"] + \";\" + tempd[\"title\"] + \";\" + tempd[\"text\"] + \";\" + tempd[\"label\"] + \";\" +  tempd[\"reply\"]\n",
    "            file.write(line + \"\\n\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(i, e)\n",
    "        \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trust_juicer(tp_url, \"trust_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex 2\n",
    "\n",
    "Lets make things more complicated. Start with the code from exercise 1 and make sure you get the link to the next page. Then, modify and use the function from exercise 1 to extract five pages of reviews. Find a way to save all the information in a single dictionary and in a unique .csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup \n",
    "\n",
    "tp_url = \"https://www.trustpilot.com/review/www.nzymes.com\"\n",
    "src = requests.get(tp_url).text\n",
    "soup = BeautifulSoup(src)\n",
    "\n",
    "def trust_juicer(u, path, index):\n",
    "    source_ = requests.get(u).text\n",
    "    soup = BeautifulSoup(source_)\n",
    "    res_ = soup.find_all(\"article\")\n",
    "    file = open(path, \"w\", encoding=\"utf-8\")\n",
    "    next_page = soup.find(\"a\", class_=\"next-page\")['href']\n",
    "    d = {}\n",
    "    for t in res_:\n",
    "        try:\n",
    "            tempd = {}\n",
    "            text = t.find(\"p\").get_text()\n",
    "            label = t.find(\"div\", class_= \"star-rating star-rating--medium\").find(\"img\").attrs[\"alt\"][0]\n",
    "            name = t.find(\"div\", class_=\"consumer-information__name\").get_text()\n",
    "            location = t.find(\"div\", class_=\"consumer-information__location\").find(\"span\").get_text()\n",
    "            n_of_reviews = t.find(\"div\", class_=\"consumer-information__review-count\").find(\"span\").get_text()\n",
    "            title = t.find(\"h2\", class_=\"review-content__title\").get_text()\n",
    "            \n",
    "            try:\n",
    "                reply = t.find(\"div\", class_=\"brand-company-reply__content\").get_text()\n",
    "            except Exception as e:\n",
    "                reply = \"None\"\n",
    "            \n",
    "            tempd[\"location\"] = location\n",
    "            tempd[\"n_of_reviews\"] = n_of_reviews\n",
    "            tempd[\"name\"] = name.strip()\n",
    "            tempd[\"title\"] = title.strip()\n",
    "            tempd[\"text\"] = text.strip()\n",
    "            tempd[\"label\"] = label\n",
    "            tempd[\"reply\"] = reply.strip()\n",
    "            \n",
    "            d[index] = tempd\n",
    "            index += 1\n",
    "            \n",
    "            line = tempd[\"name\"] + \";\" + tempd[\"location\"] + \";\" + tempd[\"n_of_reviews\"] + \";\" + tempd[\"title\"] + \";\" + tempd[\"text\"] + \";\" + tempd[\"label\"] + \";\" +  tempd[\"reply\"]\n",
    "            file.write(line + \"\\n\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(i, e)\n",
    "        \n",
    "    return d, next_page, index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trust_data = {}\n",
    "index = 0\n",
    "d, next_page, index =  trust_juicer(tp_url, \"trust_data.csv\", index)\n",
    "trust_data.update(d)\n",
    "for i in range(4):\n",
    "    d, next_page, index =  trust_juicer(\"https://www.trustpilot.com\" + next_page, \"trust_data.csv\", index)\n",
    "    trust_data.update(d)\n",
    "    \n",
    "trust_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex 3\n",
    "\n",
    "Rev's website contains transcripts of oral speeches of various kinds. Create a function that takes a url as an argument and that is able to extract a complete dialogue. Once the text is obtained, use regular expressions to clean it up. In particular, remove:\n",
    "- the name of the interlocutor;\n",
    "- time indications;\n",
    "- numbers (watch out for percentages and currencies);\n",
    "- alphanumeric codes. \n",
    "\n",
    "In addition, discard all sentences with less than 100 characters. Save the cleaned text into a text file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "rev_url = \"https://www.rev.com/blog/transcripts/apple-event-september-2021-transcript-new-iphone-13-apple-watch-ipads\"\n",
    "\n",
    "def rev_scraper(url_):\n",
    "    source = requests.get(url_)\n",
    "    soup = BeautifulSoup(source.text)\n",
    "\n",
    "    # scraping all the dialogue\n",
    "    text = soup.find(\"div\", class_=\"fl-callout-text\").find_all(\"p\")\n",
    "    text = [p.get_text() for p in text]\n",
    "    \n",
    "    file = open(\"cleaned_text.txt\", \"w\", encoding=\"utf-8\")\n",
    "    for p in text:\n",
    "        if len(p) > 100:\n",
    "            speech = p\n",
    "            # removing name of the interlocutor and time indications \n",
    "            if re.search(r\"^\\w+(\\s\\w+)?:\\s\\(.*\\)\\n\", speech, re.IGNORECASE):\n",
    "                speech = re.sub(r\"^\\w+(\\s\\w+)?:\\s\\(.*\\)\\n\", \"\", speech, re.IGNORECASE)\n",
    "            # removing numbers \n",
    "            if re.search(r\"\\$?\\d+%?(\\.\\d+)?(\\s|,)\", speech):\n",
    "                speech = re.sub(r\"\\$?\\d+%?(\\.\\d+)?(\\s|,|\\.)\", \"\", speech)\n",
    "            # removing alphanumeric codes (ex. \"5G\", \"4K\")\n",
    "            if re.search(r\"([A-z]+[\\d]+)+([A-z]+)?|([\\d]+[A-z]+)+([\\d]+)?\", speech):\n",
    "                speech = re.sub(r\"([A-z]+[\\d]+)+([A-z]+)?|([\\d]+[A-z]+)+([\\d]+)?\", \"\", speech)    \n",
    "            file.write(speech + \"\\n\\n\")\n",
    "            \n",
    "rev_scraper(rev_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex 4\n",
    "Let us make a slight modification to the function of the previous exercise: after extracting all the speeches from the page's HTML, the function will save them in a dictionary. This dictionary will have the name of the interlocutor as a key. The dictionary values must be a tuple whose first element is the moment of the recording when the interlocutor is speaking and the second element is the transcription of what is being said. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "rev_url = \"https://www.rev.com/blog/transcripts/apple-event-september-2021-transcript-new-iphone-13-apple-watch-ipads\"\n",
    "\n",
    "def rev_scraper(url_):\n",
    "    source = requests.get(url_)\n",
    "    soup = BeautifulSoup(source.text)\n",
    "\n",
    "    # scraping all the dialogue\n",
    "    text = soup.find(\"div\", class_=\"fl-callout-text\").find_all(\"p\")\n",
    "    text = [p.get_text() for p in text]\n",
    "    \n",
    "#     file = open(\"cleaned_text.txt\", \"w\", encoding=\"utf-8\")\n",
    "    data = {}\n",
    "    for p in text:\n",
    "        if len(p) > 100:\n",
    "            # splitting the speech using the time indication\n",
    "            # type(speech) : list \n",
    "            speech = re.split(r\"(:\\s\\(.*\\)\\n)\", p)\n",
    "            speacker_name = speech[0]\n",
    "            time_indication = speech[1] \n",
    "            transcript = speech[2]\n",
    "            time_indication = re.sub(r\":\\s\\((.*)\\)\\n\", r\"\\1\", time_indication)\n",
    "    \n",
    "            # removing numbers \n",
    "            if re.search(r\"\\$?\\d+%?(\\.\\d+)?(\\s|,)\", transcript):\n",
    "                transcript = re.sub(r\"\\$?\\d+%?(\\.\\d+)?(\\s|,|\\.)\", \"\", transcript)\n",
    "            # removing alphanumeric codes (ex. \"5G\", \"4K\")\n",
    "            if re.search(r\"([A-z]+[\\d]+)+([A-z]+)?|([\\d]+[A-z]+)+([\\d]+)?\", transcript):\n",
    "                transcript = re.sub(r\"([A-z]+[\\d]+)+([A-z]+)?|([\\d]+[A-z]+)+([\\d]+)?\", \"\", transcript)\n",
    "            \n",
    "            if speacker_name in data:\n",
    "                data[speacker_name].append((time_indication,transcript))\n",
    "            else:\n",
    "                data[speacker_name] = [(time_indication,transcript)]\n",
    "    return data\n",
    "rev_scraper(rev_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex 5\n",
    "\n",
    "Build your calculator using regular expressions. Thus, write a function that takes as input an operation defined as a string of the kind: \"10+12\", \"10*12\" etc. Using regular expressions, extract the operands and figure out which operation should be applied based on the symbol that divides them. The function must return the result. \n",
    "Using the function you have just defined, use the strings contained in the operations list in order to obtain a list identical to the results one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addition(a,b):\n",
    "    return a+b\n",
    "\n",
    "\n",
    "def subtraction(a,b):\n",
    "    return a-b\n",
    "\n",
    "\n",
    "def division(a,b):\n",
    "    try: \n",
    "        return a/b\n",
    "    except ZeroDivisionError as e:\n",
    "        print(f\"You cannot divide a number by 0!\")\n",
    "        \n",
    "        \n",
    "def multiplication(a,b):\n",
    "    return a*b\n",
    "\n",
    "def calculator(operation):\n",
    "    numbers = re.split(r\"\\D\", operation)\n",
    "    if re.search(\"\\+\", operation):\n",
    "        return addition(int(numbers[0]),int(numbers[1]))\n",
    "    if re.search(\"\\-\", operation):\n",
    "        return subtraction(int(numbers[0]),int(numbers[1]))\n",
    "    if re.search(\":\", operation):\n",
    "        return division(int(numbers[0]),int(numbers[1]))\n",
    "    if re.search(\"\\*\", operation):\n",
    "        return multiplication(int(numbers[0]),int(numbers[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "operations = [\"10+12\",\"12-10\",\"3*2\",\"6:3\"]\n",
    "results = [calculator(op) for op in operations]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
