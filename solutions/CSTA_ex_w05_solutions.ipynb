{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EX 1\n",
    "### Scrape an article from ProPubblica\n",
    "\n",
    "Define a function that takes a url of an article from the ProPubblica website and extracts:\n",
    "  - the title\n",
    "  - the \"abstract\", i.e: the text between the name of the author and the title \n",
    "  - the author\n",
    "  - the publication date\n",
    "  - the text in the main body\n",
    "  \n",
    "As usual, the output is a dictionary. Use the element inspector in your web console to get the right information. All the html elements can be retrieved using the selenium.get() and the BeautifulSoup() methods. \n",
    "\n",
    "![ProPublica](../img/propublica.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "driver = webdriver.Chrome(executable_path= \"chromedriver.exe\")\n",
    "sample_article = \"https://www.propublica.org/article/dr-death-christopher-duntsch-a-surgeon-so-bad-it-was-criminal\"\n",
    "art = \"https://www.propublica.org/article/q-and-a-can-a-divided-europe-handle-the-refugee-crisis\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete this function\n",
    "\n",
    "def juice_pro_publica(a_url):\n",
    "    \"\"\"Insert description here\"\"\"\n",
    "    source = \"\"\n",
    "    soup = \"\"\n",
    "    d = {}\n",
    "    d[\"Title\"] = \"\"\n",
    "    d[\"Abstract\"] = \"\" \n",
    "    d[\"Author\"] = \"\"\n",
    "    d[\"Time\"] = \"\"\n",
    "    d[\"text\"] = []\n",
    "    \n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def juice_pro_publica(driver, url):\n",
    "\n",
    "    driver.get(url)\n",
    "    print(f'Waiting 5 seconds before scraping the article\\n>>> {url}')\n",
    "    time.sleep(5)\n",
    "    soup = BeautifulSoup(driver.page_source)\n",
    "    d = {}\n",
    "    try: \n",
    "        header = soup.find(\"header\", class_=\"article-header\")\n",
    "        article = soup.find(\"div\", class_=\"article-wrap\")\n",
    "\n",
    "        title = header.find(\"h2\", class_=\"hed\").get_text()\n",
    "        try: \n",
    "            abstract = header.find(\"p\", class_=\"dek\").get_text()\n",
    "        except:\n",
    "            abstract = None\n",
    "        try:\n",
    "            author = header.find(\"span\", class_=\"name\").get_text()\n",
    "        except:\n",
    "            author = header.find(\"a\", class_=\"name\").get_text()\n",
    "\n",
    "        date = header.find(\"time\", class_=\"timestamp\").get_text()\n",
    "        text_article = [p.get_text().strip() for p in article.find_all(\"p\")]\n",
    "\n",
    "        d[\"Title\"] = title\n",
    "        d[\"Abstract\"] = abstract \n",
    "        d[\"Author\"] = author\n",
    "        d[\"Time\"] = date\n",
    "        d[\"text\"] = text_article\n",
    "            \n",
    "    except AttributeError as e:\n",
    "        print(f\"ERROR in {url}: problems in scraping all the info!\")\n",
    "        error = f'error in {url}'\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "juice_pro_publica(driver, sample_article)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex 2\n",
    "\n",
    "On the ProPublica website it is possible to search for all articles related to a search key. Try to do several searches by changing the key each time and see how the url changes. Then write a function that takes a search key as an argument and returns a list of all the related links shown on the results page.\n",
    "\n",
    "\n",
    "Search button:\n",
    "\n",
    "![ProPublica2](../img/propublica_2.png)\n",
    "\n",
    "Results page:\n",
    "\n",
    "![ProPublica3](../img/propublica_3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def get_hrefs_to_articles(driver,key):\n",
    "    \n",
    "    \"\"\"Takes a word, points the driver to the propublica webpage that contains the search \n",
    "    results for that word, collects the hrefs from the results   \"\"\"\n",
    "    \n",
    "    base_url = \"https://www.propublica.org/search?qss=\" + key.replace(\" \", \"+\")\n",
    "    driver.get(base_url)\n",
    "    # New line in the function: it forces Python to wait before executing the next lines of code\n",
    "    print(f'Waiting 5 seconds before scraping all the links for the key \"{key}\"')\n",
    "    time.sleep(5)\n",
    "    rs =BeautifulSoup(driver.page_source).find_all(\"a\", class_=\"gs-title\")\n",
    "    out_= []\n",
    "    for element in rs:\n",
    "        try:\n",
    "#           if re.search(\"\"\"^https://www.propublica.org/article/\"\"\", element[\"href\"]):  --> regex version  \n",
    "            if element[\"href\"].startswith(\"https://www.propublica.org/article/\"):\n",
    "                out_.append(element[\"href\"])\n",
    "        except KeyError:\n",
    "            pass\n",
    "    out_ = list(set(out_))        \n",
    "    return out_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_hrefs_to_articles(driver,\"italy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex 3\n",
    "\n",
    "Combine the functions of the two previous exercises. Write a function that, given a search key, is able to extract and save in a dictionary the information (title, abstract, author, date, and text body) of the articles related to the search key. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = \"italy\"\n",
    "\n",
    "def get_all_articles(driver,key_):\n",
    "    links = get_hrefs_to_articles(driver,key)\n",
    "    dataset = {key : []}\n",
    "    for url in links:\n",
    "        dataset[key].append(juice_pro_publica(driver,url))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_all_articles(driver,key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex 4\n",
    "\n",
    "Let's make things even more complicated.Write a function that can extract the information (title, abstract, author, date, and text body) contained in the articles related to each of the search keys in the list below.  Save all the information in a dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = [\"italy\", \"Apple\", \"Twitter\", \"Obama\", \"Bill Gates\"]\n",
    "\n",
    "dataset = {}\n",
    "for key in keys:\n",
    "    d = get_all_articles(driver,key)\n",
    "    dataset.update(d)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
