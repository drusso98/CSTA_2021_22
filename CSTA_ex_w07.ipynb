{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex 1\n",
    "\n",
    "In the previous weeks you have seen how to tokenize a text using regular expressions, and how to get useful information, such as lemmas and POS, using already tagged knowledge bases. In the last lesson, you were introduced to the use of existing tools that provide an efficient and intuitive pipeline for obtaining the same information with just a few lines of code. One such tool is Stanza.\n",
    "\n",
    "Let's put the pieces together and try to build useful functions for processing text with Stanza. After importing Stanza correctly, choose a language you want to work with (you can see the supported languages <a href=\"https://stanfordnlp.github.io/stanza/available_models.html\"> here</a>) and download the appropriate model. Then write two functions. The first will take as argument the path to a text file written in the language of your choice (you can download it from Gutenberg or use one of your choice). The function must save the result of the Stanza pipeline (the input of which will be the last 10000 characters of your text) as a JSON file and return the corresponding dictionary. The second function is a variation of the <i>json_to_tokens(...)</i> function already written in class. Modify it so that it also returns a dictionary whose keys will be the tokens and the related values will consist of a dictionary with the following information: lemma, upos, feats.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_to_tokens(path_, keys = [\"text\"] ):\n",
    "    out = []  \n",
    "    jin = json.load(open(path_))\n",
    "    for sent in jin:\n",
    "        temps = [] \n",
    "    for word in sent:\n",
    "        tword = []\n",
    "        for key in keys: \n",
    "            try:\n",
    "                tword.append(word[key])\n",
    "            except KeyError:\n",
    "                pass\n",
    "        temps.append(tword) \n",
    "    out.append(temps) \n",
    "return out "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex 2\n",
    "\n",
    "From the Git profile of <a href=\"https://github.com/UniversalDependencies\">UniversalDependencies</a> choose two repositories of your interest and download them with the command \"git clone\" after having positioned yourself in the folder of interest directly using the terminal.  Next, write a function that takes as argument the path to a Git folder. Using the \"os\" module, extract the path to all the \".conllu\" files and process them to obtain for each file a dictionary whose keys will be tokens, and the ralated values will be a dictionary with the following information: lemma, and pos. Save all the dictionaries in a list and return it. Do this for both Git directories. At the end of the process make sure you get a list containing all the lists of the various dictionaries. Would you be able to attach the name of the repository you got the data from to each list?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex 3\n",
    "\n",
    "On this <a href=\"https://www.tripadvisor.it/Tourism-g194889-Rovereto_Province_of_Trento_Trentino_Alto_Adige-Vacations.html\">page</a> you will find a number of places in Rovereto which have been reviewed by users. Using selenium and BeautifoulSoup, scrape the url and for each item extract the reviews at the bottom of each page.In particular, for each review extract and save in a dictionary the following information: name of the reviewer, location (if any), number of reviews, rating, number of likes, date, title, and text of the review. The final dataset will consist of a dictionary that has the name of the location/activity as key and dictionaries with the information from the different reviews as values.  Divide up the code in an appropriate and functional way, making use of several functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex 4 \n",
    "\n",
    "The following iterative sequence is defined for the set of positive integers:\n",
    "\n",
    "<br><center> n → n/2 (n is even)\n",
    "\n",
    "<center>n → 3n + 1 (n is odd)\n",
    "\n",
    "<br>Using the rule above and starting with 13, we generate the following sequence:\n",
    "\n",
    "<br><center> 13 → 40 → 20 → 10 → 5 → 16 → 8 → 4 → 2 → 1\n",
    "    \n",
    "    \n",
    "<br>It can be seen that this sequence (starting at 13 and finishing at 1) contains 10 terms. Although it has not been proved yet (Collatz Problem), it is thought that all starting numbers finish at 1.\n",
    "\n",
    "Which starting number, under one million, produces the longest chain?\n",
    "\n",
    "NOTE: Once the chain starts the terms are allowed to go above one million."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
