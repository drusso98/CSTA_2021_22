{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C17FGGJJfdm6"
   },
   "source": [
    "# Ex 1\n",
    "\n",
    "Take a text from the Gutenberg Project and clean it (.i.e.: remove the GP messages from the head and the tail of the text using any text editor). Once you have a clean text in your language of choice, write a function that takes the path to that file as input (the path itself is a string, as you know) and returns a list of lists, where every smaller list represents a sentence with its tokens. Something like:\n",
    "\n",
    "```python\n",
    "input_text = \"I am blue. You are green!\"\n",
    "output = [[\"I\",\"am\",\"blue\",\".\",],[\"You\",\"are\",\"green\",\"!\" ]]\n",
    "```\n",
    "\n",
    "In order to do this, you will have to build two tokenizers (you will decide whether or not to do that in the same function): one that takes care of the sentence tokenization and one that takes care of the word tokenization. You will use the \"re\" module to build the tokenizers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q3Gt05XjeklR"
   },
   "source": [
    "# Ex 2\n",
    "\n",
    "Given the better dictionary structure that we created try to determine how precise out current lemmatization approach can be. More precisely, we now have a gold-standard dataset (the connlu file in the [github repository](https://github.com/UniversalDependencies/UD_English-EWT) seen in class) which contains information about the tokens, the POS and lemmas in a text. Use the token + POS information to obtain a lemmatization via the [ENGLAFF](http://redac.univ-tlse2.fr/lexiques/englaff.html)-based lemmatizer; note that you need to rewrite the function \"lemmatizer\" to fit the new data structure in the \"better_d\" variable, and that you have to find a way to convert the POS tag found in the conllu file to match the one found in the ENGLAFF file (or viceversa). How can you measure the precision of our tool?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex 3\n",
    "\n",
    "Let's create a dataset consisting of song lyrics by scraping https://www.azlyrics.com. You can access the full list of artists in alphabetical order by browsing the top of the site (is there an easier way to go directly to the page of interest?). Using requests or selenium, in combination with BeautifoulSoup, accomplish the following tasks:\n",
    "- Create a function that, given a letter of the alphabet, returns a list of all the artists present on the site whose name begins with that letter and the respective link to their page. Save the information in a list of tuples in which the first element is the name of the artist and the second one is the link to her\\his page;\n",
    "- Write a function that, given the link to the artist webpage, returns the list of all her/his songs present on the website and the respective link to the lyrics. Save the information in a list of tuples in which the first element is the title of the song and the second the link to the lyrics;\n",
    "- Write a function that, given a link to the lyrics page, returns the lyrics of the song;\n",
    "\n",
    "Write a function that takes a letter as input and returns a dictionary whose keys are the names of all the artists on the website starting with that letter, and as a value a dictionary structured as follows: as keys it will have the names of the songs, and as values it will have another dictionary in which you will store the link and the lyrics of the song. The final structure of your dataset should be similar to the one below.\n",
    "\n",
    "```python\n",
    "dataset = {[artist name]: {[song title]: {[link]: str,\n",
    "                                          [lyrics]: str},\n",
    "                           [song title]: {[link]: str,\n",
    "                                          [lyrics]: str},\n",
    "                           ...\n",
    "                                                            },\n",
    "           \n",
    "           [artist name]: {...},\n",
    "          \n",
    "           ...\n",
    "           \n",
    "          }\n",
    "```\n",
    "\n",
    "Are you able to retrieve the lyrics of a song of your choice with a single line of code?\n",
    "\n",
    "Finally, try to store artists and related lyrics using more than just one letter.\n",
    "\n",
    "<b>N.B.</b> Remember to use time.sleep() before \"getting\" the website:\n",
    "```python\n",
    "import time\n",
    "...\n",
    "time.spleep(5) # 5-second wait\n",
    "requests.get(...)\n",
    "```\n",
    "or\n",
    "\n",
    "```python\n",
    "import time\n",
    "...\n",
    "time.spleep(5) # 5-second wait\n",
    "driver.get(...)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex 4\n",
    "\n",
    "Using names.txt (data folder), a 46K text file containing over five-thousand first names, begin by sorting it into alphabetical order. Then working out the alphabetical value for each name, multiply this value by its alphabetical position in the list to obtain a name score.\n",
    "\n",
    "For example, when the list is sorted into alphabetical order, COLIN, which is worth 3 + 15 + 12 + 9 + 14 = 53, is the 938th name in the list. So, COLIN would obtain a score of 938 Ã— 53 = 49714.\n",
    "\n",
    "What is the total of all the name scores in the file?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "CSTA_ex_w01.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
